{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "This notebook uses the unity ML environment and trains a Multi-Agent Twin Delayed Deep Deterministic Policy Gradient (MATD3) agent to solve the Tennis environment.\n",
    "\n",
    "DDPG is an actor-critic method closely related to a Deep Q-Network (DQN). Sometimes called an approximate DQN, a DDPQ learns a function that chooses the action that maximizes the Q-values of the next state, similar to a DQN. TD3 adds to the DDPG architecture with methods that improve stability. MATD3 extends TD3 to work with environments that have multiple agents, as we'll see here.\n",
    "\n",
    "First, the modules needed for the project are loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Import common/pip libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from collections import deque\n",
    "from unityagents import UnityEnvironment\n",
    "\n",
    "# Import custom modules\n",
    "from td3_agent import MultiAgent\n",
    "from model import Actor, Critic"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you want to use a GPU, make sure torch is running correctly. Otherwise, skip this step."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True True True <torch.cuda.device object at 0x000001788F2953C8> NVIDIA GeForce RTX 3080 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# torch gpu compatibility test\n",
    "print(torch.cuda.is_available()==True,\n",
    "torch.cuda.device_count()==1,\n",
    "torch.cuda.current_device()==0,\n",
    "torch.cuda.device(0),\n",
    "torch.cuda.get_device_name(0))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Select device to run for training."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Uncomment the device you want to use.\n",
    "#device = 'cuda:0'\n",
    "device = 'cpu'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. Start the Environment\n",
    "\n",
    "If the python environment specifications are correct, then this cell should return no error. Otherwise, reference the project README and env.txt to ensure you have all the necessary packages. If the errors persist, try building an environment straight from the env.txt using `pip install -r env.txt`.\n",
    "\n",
    "If the unity environment fails to load, make sure you have the `Reacher_Data` folder in the same directory as this notebook. Also verify the version number of the `unityagents` package (0.4.0) and try using version 3.6.3 of `python`.\n",
    "\n",
    "If you want to train the agent and don't want to see it do anything, comment out line 2 in the cell below."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "unity_env = UnityEnvironment(file_name='Tennis.exe'\n",
    "                       ,no_graphics=False\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = unity_env.brain_names[0]\n",
    "brain = unity_env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1.  If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01.  Thus, the goal of each agent is to keep the ball in play.\n",
    "\n",
    "The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping.\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.65278625 -1.5\n",
      " -0.          0.          6.83172083  6.         -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = unity_env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3. (Optional) Take Random Actions in the Environment\n",
    "\n",
    "Run the following code block to see what it looks like when the agent behaves randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score (max over agents) from episode 1: 0.0\n",
      "Score (max over agents) from episode 2: 0.0\n",
      "Score (max over agents) from episode 3: 0.0\n",
      "Score (max over agents) from episode 4: 0.0\n",
      "Score (max over agents) from episode 5: 0.09000000171363354\n"
     ]
    }
   ],
   "source": [
    "# Run a random agent\n",
    "for i in range(1, 6):                                      # play game for 5 episodes\n",
    "    env_info = unity_env.reset(train_mode=False)[brain_name]     # reset the environment\n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    while True:\n",
    "        actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "        actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "        env_info = unity_env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "    print('Score (max over agents) from episode {}: {}'.format(i, np.max(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Don't worry about where the agent is after the random actions. The environment will reset before we begin training."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train the Agent\n",
    "\n",
    "The following cell trains the agent using the MATD3 architecture described in `report.md`."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Initialize the agent.\n",
    "agent = MultiAgent(state_size=state_size, action_size=action_size, number_of_agents=num_agents,\n",
    "                   actor=Actor, critic=Critic, device=device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 0.00\n",
      "Episode 200\tAverage Score: 0.00\n",
      "Episode 300\tAverage Score: 0.00\n",
      "Episode 400\tAverage Score: 0.01\n",
      "Episode 500\tAverage Score: 0.00\n",
      "Episode 600\tAverage Score: 0.00\n",
      "Episode 700\tAverage Score: 0.01\n",
      "Episode 800\tAverage Score: 0.05\n",
      "Episode 900\tAverage Score: 0.04\n",
      "Episode 1000\tAverage Score: 0.05\n",
      "Episode 1100\tAverage Score: 0.06\n",
      "Episode 1200\tAverage Score: 0.06\n",
      "Episode 1300\tAverage Score: 0.06\n",
      "Episode 1400\tAverage Score: 0.08\n",
      "Episode 1500\tAverage Score: 0.10\n",
      "Episode 1600\tAverage Score: 0.09\n",
      "Episode 1700\tAverage Score: 0.14\n",
      "Episode 1800\tAverage Score: 0.20\n",
      "Episode 1900\tAverage Score: 0.33\n",
      "Episode 2000\tAverage Score: 0.27\n",
      "Episode 2100\tAverage Score: 0.48\n",
      "Episode 2109\tAverage Score: 0.50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.legend:No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2110\tAverage Score: 0.52\n",
      "Environment solved in 2010 episodes!\tAverage Score: 0.52\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Env(object):\n",
    "    \"\"\"\n",
    "    Helper Class for translating Unity env to gym env.\n",
    "    \"\"\"\n",
    "    def __init__(self, unity_env, train_mode=True):\n",
    "        \"\"\"Constructor for environment helper class.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            unity_env (object): unity environment object\n",
    "            train_mode (bool): start the unity environment in training mode\n",
    "        \"\"\"\n",
    "        self.env = unity_env\n",
    "        self.brain_name = self.env.brain_names[0]\n",
    "        self.brain = self.env.brains[self.brain_name]\n",
    "        self.action_size = self.brain.vector_action_space_size\n",
    "        self.env_info = self.env.reset(train_mode=train_mode)[self.brain_name]\n",
    "        self.state_size = len(self.env_info.vector_observations[0])\n",
    "        self.train_mode = train_mode\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the unity environment state\n",
    "        \"\"\"\n",
    "        self.env_info = self.env.reset(train_mode=self.train_mode)[self.brain_name]\n",
    "        return self.env_info.vector_observations\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Take a step in the unity environment.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            action (int): action selected by agent\n",
    "        \"\"\"\n",
    "        self.env_info = self.env.step(action)[self.brain_name]        # send the action to the environment\n",
    "        next_state = self.env_info.vector_observations   # get the next state\n",
    "        reward = self.env_info.rewards                  # get the reward\n",
    "        done = self.env_info.local_done\n",
    "        return next_state, reward, done, None\n",
    "\n",
    "\n",
    "env = Env(unity_env)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def td3(n_episodes=2000, max_t=1000, target_score=200, num_agents=2):\n",
    "    \"\"\"Run a TD3 training loop.\n",
    "\n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        target_score (float): stop the training early if the average score over a window\n",
    "                              is higher than target_score\n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    total_steps_taken = 0\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "        score = np.zeros(num_agents)\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, _ = env.step(action.cpu().data.numpy())\n",
    "            agent.step(state, action.cpu().data.numpy(), reward, next_state, done, total_steps_taken)\n",
    "            state = next_state\n",
    "            score += np.array(reward)\n",
    "            total_steps_taken += 1\n",
    "            if np.any(done):\n",
    "                break\n",
    "\n",
    "        scores_window.append(np.max(score))       # save most recent score for early stopping\n",
    "        scores.append(score)              # save most recent score for recording all scores\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        if np.mean(scores_window)>=target_score:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            agent.save()\n",
    "            break\n",
    "\n",
    "    return scores\n",
    "\n",
    "# Begin training\n",
    "scores = td3(n_episodes=10000, target_score=0.5)\n",
    "\n",
    "# separate scores for plotting\n",
    "max_scores = [max(score) for score in scores]\n",
    "mean_max_scores = [np.mean([max(score) for score in scores[i:(i+100)]]) for i in range(len(scores))]\n",
    "a1_scores = [score[0] for score in scores]\n",
    "a2_scores = [score[1] for score in scores]\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), max_scores)\n",
    "plt.plot(np.arange(len(scores)), mean_max_scores)\n",
    "plt.legend()\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()\n",
    "plt.savefig('avg_reward.png')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluate\n",
    "\n",
    "As shown previously, the agent solves the environment in ~2000 episodes. If you have the unity window open and enabled in the environment, then you can run the cell below to watch your trained agent."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "array([4.29000006, 3.91000006])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the weights from file\n",
    "agent.agents[0].actor_local.load_state_dict(torch.load('checkpoints/agent_0_actor_checkpoint.pth'))\n",
    "agent.agents[1].actor_local.load_state_dict(torch.load('checkpoints/agent_1_actor_checkpoint.pth'))\n",
    "\n",
    "# Observe trained agent\n",
    "env_info = unity_env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "state = env_info.vector_observations            # get the current state\n",
    "score = np.zeros(num_agents)                                     # initialize the score\n",
    "\n",
    "for _ in range(10):\n",
    "    while True:\n",
    "        action = agent.act(state, add_noise=True)        # select an action\n",
    "        env_info = unity_env.step(action.cpu().data.numpy())[brain_name]        # send the action to the environment\n",
    "        next_state = env_info.vector_observations      # get the next state\n",
    "        reward = env_info.rewards                      # get the reward\n",
    "        done = env_info.local_done                     # see if episode has finished\n",
    "        score += np.array(reward)                                # update the score\n",
    "        state = next_state                             # roll over the state to next time step\n",
    "        if np.any(done):                                       # exit loop if episode finished\n",
    "            break\n",
    "\n",
    "score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Don't forget to close the environment when you're done."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# close environment\n",
    "unity_env.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}